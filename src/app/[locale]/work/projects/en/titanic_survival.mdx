# Predicting Titanic Survival: A Machine Learning Investigation

## 1. Introduction

The Titanic disaster of 1912 remains one of the most infamous maritime tragedies in history. Among the 2,224 passengers and crew onboard, only around 700 survived. The goal of this investigation is to analyze the factors that influenced survival rates and predict whether a passenger would survive using machine learning techniques.

### Objective

The primary aim of this study is to:
1. Perform exploratory data analysis (EDA) to identify patterns and trends.
2. Build a machine learning model to predict survival based on passenger attributes.

### Hypothesis

We hypothesize that:
- Women and children were more likely to survive.
- Passengers in higher classes had better chances of survival due to better access to lifeboats.

---

## 2. Libraries and Dataset Overview

### Libraries Used
- **`pandas`**: Data manipulation and cleaning.
- **`numpy`**: Numerical computations.
- **`matplotlib` & `seaborn`**: Visualization of data distributions and relationships.
- **`sklearn`**: Machine learning models and evaluation metrics.

### Dataset Description
The dataset contains information about Titanic passengers, including:
- **Demographic features**: `Age`, `Sex`, `Embarked`.
- **Socioeconomic features**: `Pclass` (Passenger class), `Fare`.
- **Family-related features**: `SibSp` (siblings/spouses), `Parch` (parents/children).
- **Survival status**: `Survived` (0 = No, 1 = Yes).

The dataset is divided into training and testing sets, enabling us to train our models and evaluate their performance on unseen data.

---

## 3. Exploratory Data Analysis (EDA)

### Visualizing Key Features

#### Age Distribution
```python
sns.histplot(train_df['Age'], kde=True, bins=30, color='blue')
plt.title('Age Distribution')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()
```

![Age Distribution](./images/age_distribution.png "Age Distribution")

**Analysis**: The majority of passengers were between 20 and 40 years old, indicating a younger demographic onboard. Survival chances for different age groups can provide deeper insights.

#### Survival by Gender
```python
sns.barplot(data=train_df, x='Sex', y='Survived', ci=None, palette='coolwarm')
plt.title('Survival by Gender')
plt.xlabel('Gender (0=Female, 1=Male)')
plt.ylabel('Survival Rate')
plt.show()
```

![Survival by Gender](./images/survival_by_gender.png "Survival by Gender")

**Analysis**: Female passengers had a significantly higher survival rate compared to males. This supports historical accounts of "women and children first" policies during the evacuation.

#### Survival by Class
```python
sns.barplot(data=train_df, x='Pclass_1', y='Survived', ci=None, palette='coolwarm')
plt.title('Survival by Passenger Class')
plt.xlabel('Class (1=First, 2=Second, 3=Third)')
plt.ylabel('Survival Rate')
plt.show()
```

![Survival by Class](./images/survival_by_class.png "Survival by Class")

**Analysis**: First-class passengers had the highest survival rates, while third-class passengers fared the worst. This highlights socioeconomic disparities in access to lifeboats.

#### Correlation Heatmap
```python
plt.figure(figsize=(10, 8))
sns.heatmap(train_df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()
```

![Correlation Heatmap](./images/correlation_heatmap.png "Correlation Heatmap")

**Analysis**: Fare and class exhibit notable correlations with survival, while age shows a weaker correlation. This suggests that financial and social status were critical factors.

---

## 4. Feature Engineering

Feature engineering transforms raw data into meaningful features for the machine learning model. Key steps included:

### Addressing Family Size
- **Problem**: Zero or unexpected values in `Family_size` could lead to errors during analysis.
- **Solution**: Replace zero values with 1 to ensure meaningful calculations for features like `Fare_Per_Person`.

### Interaction Terms
- **`Fare_Per_Person`**: Calculates the fare paid per family member to account for shared resources.
- **`Age_Class`**: Multiplies age by passenger class to capture the interplay between age and social hierarchy.

### Advanced Transformations
- **Log Transformation**: Reduces the impact of extreme fare values.
- **Polynomial Features**: Captures non-linear relationships, such as squared impacts of age and fare.

```python
train_df['Log_Fare'] = np.log1p(train_df['Fare'])
train_df['Fare_Squared'] = train_df['Fare'] ** 2
train_df['Age_Squared'] = train_df['Age'] ** 2
```

![Feature Engineering Summary](./images/feature_engineering.png "Feature Engineering")

**Impact**: These transformations enhance the model's ability to identify nuanced patterns in the data.

---

## 5. Machine Learning Methodology

### Modeling Approach
We used a stacking classifier that combines multiple models:
- **`RandomForestClassifier`**: Captures non-linear patterns and handles missing data well.
- **`GradientBoostingClassifier`**: Handles imbalanced datasets effectively and captures complex relationships.
- **`LogisticRegression`**: Acts as the final estimator to aggregate predictions from the base models.

### Why Stacking?
Stacking leverages the strengths of diverse models, allowing each to compensate for the weaknesses of others. This results in improved generalization and robustness.

---

## 6. Evaluation

### Metrics Used
- **Accuracy**: Measures the overall correctness of predictions.
- **Precision**: Indicates the proportion of positive identifications that were correct.
- **Recall**: Reflects the ability to identify all positive cases.
- **F1-Score**: Balances precision and recall into a single metric.

### Results

#### Accuracy and Classification Report
```python
accuracy = accuracy_score(y_test, y_pred_adjusted)
print("Accuracy:", accuracy)
print("\nClassification Report:\n", classification_report(y_test, y_pred_adjusted))
```

![Classification Report](./images/classification_report.png "Classification Report")

**Observation**: The model achieved an accuracy of 87%, with balanced precision and recall scores.

#### Confusion Matrix
```python
conf_matrix = confusion_matrix(y_test, y_pred_adjusted)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Survived', 'Survived'], yticklabels=['Not Survived', 'Survived'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
```

![Confusion Matrix](./images/confusion_matrix.png "Confusion Matrix")

**Analysis**: The confusion matrix reveals that the model performs well on identifying survivors but occasionally misclassifies non-survivors.

#### Precision-Recall Curve
```python
plt.figure(figsize=(8, 6))
plt.plot(recalls, precisions, marker='.', label='Stacked Model')
plt.axvline(x=np.max(recalls * precisions), color='red', linestyle='--', label='Optimal Threshold')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()
plt.show()
```

![Precision-Recall Curve](./images/precision_recall_curve.png "Precision-Recall Curve")

**Analysis**: The curve highlights the trade-off between precision and recall, with the optimal threshold maximizing the product of both metrics.

---

## 7. Results and Insights

### Key Findings
- **Gender**: Women were far more likely to survive than men, supporting the "women and children first" policy.
- **Class**: First-class passengers had a distinct survival advantage over lower-class passengers.
- **Fare and Family Size**: Higher fares and smaller family sizes positively influenced survival probabilities.

### Hypothesis Check
The results align with the hypothesis: women, children, and first-class passengers were more likely to survive. However, other factors like fare and family size also played critical roles.

---

## 8. Conclusion

### Summary
The final stacked model achieved an accuracy of 87%, demonstrating the effectiveness of feature engineering and ensemble methods. The results validate historical accounts of survival trends during the Titanic disaster.

### Limitations
- **Data Imbalance**: The imbalance in survival classes could lead to biases in predictions.
- **Feature Limitations**: Some features like `Embarked` had limited predictive power, suggesting additional data might improve performance.

### Future Work
- Explore external datasets to incorporate additional features such as passenger health or specific cabin locations.
- Test advanced models like XGBoost or neural networks for potentially higher accuracy.

---
